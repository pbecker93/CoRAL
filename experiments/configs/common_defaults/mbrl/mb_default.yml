
---
name: "DEFAULT"
repetitions: 1
iterations: 1
params:
  experiment:
    world_model: "rssm"
    agent: "li"
    seed: 0
    fully_deterministic: False
    save_interval: -1 # -1 means never
  env:
    action_repeat: 2
  policy:
    actor:
      num_layers: 3
      layer_size: 300
      activation: "ELU"
      min_std: 0.0001
      init_std: 5.0
      mean_scale: 5.0
      apply_mean_scale: True
    value:
      num_layers: 3
      layer_size: 300
      activation: "ELU"
  trainer:
    actor_learning_rate: 0.00008
    actor_weight_decay: 0.0
    value_learning_rate: 0.00008
    slow_value:
      use: True
    imagine_horizon: 15
    imagine_from_smoothed: False
    discount: 0.99
    model_learning_rate: 0.0003
    model_weight_decay: 0.0
    model_clip_norm: 10
    eval_interval: -1
    entropy:
      learning_rate: 0.0003
      clip_norm: 1.0
      bonus: 0.0
      exp_activation: False
      learn_bonus: False
      target: "auto"
    objective:
      reward_scale_factor: 1
      kl:
        scale_factor: 1.0
        free_nats: 1.0
        balanced: True
        alpha: 0.8
      inv_dyn:
        activation: "ELU"
        scale_factor: 1.0
        layer_size: 128
        num_layers: 2
  rl:
    rl_exp:
      model_updt_seq_length: 50
      model_updt_batch_size: 50
    data_collection:
      action_noise_std: 0.3
  model:
    encoder0:
      conv_depth_factor: 32
      conv_activation: "ReLU"
      dense_activation: "ELU"
      add_dense_layer: False
      num_layers: 3
      layer_size: 300
    transition:
      type: "r_rssm"
      lsd: 64
      rec_state_dim: 200
      num_layers: 2
      layer_size: 400
      min_std: 0.1
      activation: "ELU"
      num_categoricals: 32
      categorical_size: 32
    reward_decoder:
      num_layers: 3
      layer_size: 300
      activation: "ELU"
  policy_eval:
    reward_eval:
      eval_interval: 20
      num_sequences: 20
      use_mean: True
      record_vid: False
      max_sequence_length: -1